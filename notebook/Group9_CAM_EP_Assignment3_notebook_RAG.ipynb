{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aaa2fed4",
      "metadata": {
        "id": "aaa2fed4"
      },
      "source": [
        "# Env Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies on colab**"
      ],
      "metadata": {
        "id": "lrcjwj2ij2Io"
      },
      "id": "lrcjwj2ij2Io"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain>=0.2\" langchain-community langchain-openai qdrant-client \"pypdf>=4.0.0\" \"langchain-qdrant\" gdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI6o2Ns1j2Uu",
        "outputId": "6b5258fa-4dfe-4268-bbea-abc3725eaabe"
      },
      "id": "iI6o2Ns1j2Uu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain>=0.2 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pypdf>=4.0.0\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-qdrant\n",
            "  Downloading langchain_qdrant-0.2.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2) (6.0.2)\n",
            "Collecting requests<3,>=2 (from langchain>=0.2)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (1.75.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.41.0->qdrant-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain>=0.2) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain>=0.2) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain>=0.2) (0.25.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain>=0.2) (3.4.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.2) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m86.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m263.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m352.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m192.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_qdrant-0.2.1-py3-none-any.whl (24 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m234.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m741.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, portalocker, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, qdrant-client, langchain-qdrant, langchain-openai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.30 langchain-openai-0.3.33 langchain-qdrant-0.2.1 marshmallow-3.26.1 mypy-extensions-1.1.0 portalocker-3.2.0 pypdf-6.1.1 qdrant-client-1.15.1 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f947630",
      "metadata": {
        "id": "5f947630"
      },
      "outputs": [],
      "source": [
        "# https://github.com/qdrant/qdrant/releases\n",
        "# On Windows use PowerShell, on macOS use the regular Terminal.\n",
        "# On macOS you probably use \"/\" instead of \"\\\"\n",
        "# cd C:\\qdrant <- your path\n",
        "# .\\qdrant.exe <- run in the terminal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libs**"
      ],
      "metadata": {
        "id": "qIM0-dQXj-HA"
      },
      "id": "qIM0-dQXj-HA"
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import uuid\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- LangChain core\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# --- Loaders\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "\n",
        "# --- OpenAI embeddings (LangChain)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# --- Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as rest\n",
        "from langchain_qdrant import QdrantVectorStore\n"
      ],
      "metadata": {
        "id": "1j9Rwy1ajfpk"
      },
      "id": "1j9Rwy1ajfpk",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup keys and pathes**"
      ],
      "metadata": {
        "id": "qK-A5XCEkAmJ"
      },
      "id": "qK-A5XCEkAmJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# we set environment variables while the program is running\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n",
        "os.environ[\"QDRANT_URL\"] = \"https://23c58579-503f-4a87-93a0-2e5b386c65f0.europe-west3-0.gcp.cloud.qdrant.io:6333\"\n",
        "os.environ['QDRANT_API_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.KsoZCg8rz5OLcu4NazcswJbCr0psRKgjdIkXTKG9aig'\n",
        "\n",
        "\n",
        "\n",
        "DATA_DIR = Path(\"./data\")\n",
        "COLLECTION_NAME = \"my_rag_collection\"\n",
        "\n",
        "# Embeddings (OpenAI)\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"  # -small (cheaper)\n",
        "CHAT_MODEL = \"gpt-4o-mini\"\n",
        "\n"
      ],
      "metadata": {
        "id": "x_peSV5ykW7_"
      },
      "id": "x_peSV5ykW7_",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download dataset**"
      ],
      "metadata": {
        "id": "aZ5Daro80jNB"
      },
      "id": "aZ5Daro80jNB"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "URL = \"https://drive.google.com/drive/folders/1wQQUmLxxqXABXzkj1D3je9HTPOMcHgrt?usp=share_link\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "gdown.download_folder(URL, output=str(DATA_DIR), quiet=False)      # pobiera wszystkie pliki z folderu\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEk6RA1L0jWW",
        "outputId": "78482814-e0a4-4032-e32b-517e4a868546"
      },
      "id": "UEk6RA1L0jWW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1EX4S0bcJRDgh7VgjLlkmzni_VZOAiBpj NYSE_CS_2016.pdf\n",
            "Processing file 1zKCyWiseB5wcyDBoAmbfR6XIpxrrs6PK NYSE_CS_2017.pdf\n",
            "Processing file 1pZL2w-2HX9hlryndqjZTBO_3qFHLcDYy NYSE_CS_2018.pdf\n",
            "Processing file 1xlbQvR_5OQ1JxDTIg6RVC2fYpKZwVGHc NYSE_CS_2019.pdf\n",
            "Processing file 1c2oHH-ch9rRIKb019gqnfldcJOuD7oxb NYSE_CS_2023.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EX4S0bcJRDgh7VgjLlkmzni_VZOAiBpj\n",
            "To: /content/data/NYSE_CS_2016.pdf\n",
            "100%|██████████| 6.94M/6.94M [00:00<00:00, 28.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zKCyWiseB5wcyDBoAmbfR6XIpxrrs6PK\n",
            "To: /content/data/NYSE_CS_2017.pdf\n",
            "100%|██████████| 6.61M/6.61M [00:00<00:00, 28.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pZL2w-2HX9hlryndqjZTBO_3qFHLcDYy\n",
            "To: /content/data/NYSE_CS_2018.pdf\n",
            "100%|██████████| 5.91M/5.91M [00:00<00:00, 23.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xlbQvR_5OQ1JxDTIg6RVC2fYpKZwVGHc\n",
            "To: /content/data/NYSE_CS_2019.pdf\n",
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 32.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1c2oHH-ch9rRIKb019gqnfldcJOuD7oxb\n",
            "To: /content/data/NYSE_CS_2023.pdf\n",
            "100%|██████████| 3.26M/3.26M [00:00<00:00, 23.8MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/NYSE_CS_2016.pdf',\n",
              " 'data/NYSE_CS_2017.pdf',\n",
              " 'data/NYSE_CS_2018.pdf',\n",
              " 'data/NYSE_CS_2019.pdf',\n",
              " 'data/NYSE_CS_2023.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBDwsx3s1Izu",
        "outputId": "d647a83e-2cd4-4dd4-9cef-0e8fc469bea1"
      },
      "id": "wBDwsx3s1Izu",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NYSE_CS_2016.pdf  NYSE_CS_2018.pdf  NYSE_CS_2023.pdf\n",
            "NYSE_CS_2017.pdf  NYSE_CS_2019.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "675702cb",
      "metadata": {
        "id": "675702cb"
      },
      "source": [
        "# Populate vectordb (qdrant)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGHAY0Vxd4fi"
      },
      "id": "QGHAY0Vxd4fi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking params setup**"
      ],
      "metadata": {
        "id": "GPP8F80akYgN"
      },
      "id": "GPP8F80akYgN"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8a72c482",
      "metadata": {
        "id": "8a72c482"
      },
      "outputs": [],
      "source": [
        "# Chinking params\n",
        "CHUNK_SIZE = 800          # number of characters\n",
        "CHUNK_OVERLAP = 100       # number of characters\n",
        "SEPARATORS = [\"\\n\\n\", \"\\n\", \" \", \"\"]  # To be specified after analyzing the file structure. It may significantly affect the quality of RAG responses.\n",
        "\n",
        "# Batching writes (performance)\n",
        "WRITE_BATCH_SIZE = 256\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils functions**"
      ],
      "metadata": {
        "id": "1hVTx4kgkm6q"
      },
      "id": "1hVTx4kgkm6q"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9b38b8eb",
      "metadata": {
        "id": "9b38b8eb"
      },
      "outputs": [],
      "source": [
        "def load_documents(data_dir: Path) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads documents from the data_dir directory.\n",
        "    Supports PDF files.\n",
        "    Each Document has the following metadata: source, page (for PDF), filetype.\n",
        "    \"\"\"\n",
        "    docs: List[Document] = []\n",
        "\n",
        "    for p in data_dir.rglob(\"*.pdf\"):\n",
        "        loader = PyPDFLoader(str(p))\n",
        "        pdf_docs = loader.load()\n",
        "        for d in pdf_docs:\n",
        "            d.metadata = {**d.metadata, \"source\": str(p), \"filetype\": \".pdf\", \"page\": d.metadata.get(\"page\")}\n",
        "        docs.extend(pdf_docs)\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "def make_chunks(docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Chunking documents based on RecursiveCharacterTextSplitter.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        separators=SEPARATORS,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "def deterministic_id(text: str, metadata: dict) -> str:\n",
        "    \"\"\"\n",
        "    Returns a stable, deterministic UUID v5 based on the chunk content\n",
        "    and key metadata. Compliant with Qdrant requirements (UUID or int).\n",
        "    \"\"\"\n",
        "    source = str(metadata.get(\"source\", \"\"))\n",
        "    page = str(metadata.get(\"page\", \"\"))\n",
        "    text_hash = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
        "    name = f\"{source}|{page}|{text_hash}\"\n",
        "\n",
        "    return str(uuid.uuid5(uuid.NAMESPACE_URL, name))\n",
        "\n",
        "\n",
        "def ensure_collection(client: QdrantClient, collection_name: str, vector_size: int) -> None:\n",
        "    \"\"\"\n",
        "    Creates a collection if it does not exist. For simplicity: 1 vector per record, Cosine.\n",
        "    \"\"\"\n",
        "    exists = client.collection_exists(collection_name)\n",
        "    if exists:\n",
        "        return\n",
        "\n",
        "    client.recreate_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=rest.VectorParams(size=vector_size, distance=rest.Distance.COSINE),\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function**"
      ],
      "metadata": {
        "id": "ZJSE5TiOlJA2"
      },
      "id": "ZJSE5TiOlJA2"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8aa142b1",
      "metadata": {
        "id": "8aa142b1"
      },
      "outputs": [],
      "source": [
        "def populate_vector_store():\n",
        "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not openai_api_key:\n",
        "        raise RuntimeError(\"Setup OPENAI_API_KEY.\")\n",
        "\n",
        "    qdrant_url = os.environ.get(\"QDRANT_URL\")\n",
        "    qdrant_api_key = os.environ.get('QDRANT_API_KEY')\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=openai_api_key)\n",
        "\n",
        "    if EMBEDDING_MODEL == \"text-embedding-3-large\":\n",
        "        dim = 3072\n",
        "    elif EMBEDDING_MODEL == \"text-embedding-3-small\":\n",
        "        dim = 1536\n",
        "    else:\n",
        "        raise RuntimeError(\"Setup embeddings shape.\")\n",
        "\n",
        "    client = QdrantClient(\n",
        "        url=qdrant_url,\n",
        "        api_key=qdrant_api_key,\n",
        "        timeout=60,\n",
        "    )\n",
        "\n",
        "    # Load pdfs\n",
        "    if not DATA_DIR.exists():\n",
        "        raise RuntimeError(f\"Directory {DATA_DIR} does not exist. Create {DATA_DIR} and add some files!\")\n",
        "\n",
        "    base_docs = load_documents(DATA_DIR)\n",
        "    if not base_docs:\n",
        "        raise RuntimeError(f\"Empty directory {DATA_DIR}. Add some .pdf files.\")\n",
        "\n",
        "    # Chunking\n",
        "    chunks = make_chunks(base_docs)\n",
        "\n",
        "    # Idempotentne IDs\n",
        "    ids = [deterministic_id(doc.page_content, doc.metadata) for doc in chunks]\n",
        "\n",
        "    # Setup Qdrant collection\n",
        "    ensure_collection(client, COLLECTION_NAME, dim)\n",
        "    vectorstore = QdrantVectorStore(\n",
        "        client=client,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding=embeddings,\n",
        "    )\n",
        "\n",
        "    # Save chunks to Qdrant\n",
        "    for i in range(0, len(chunks), WRITE_BATCH_SIZE):\n",
        "        batch_docs = chunks[i : i + WRITE_BATCH_SIZE]\n",
        "        batch_ids = ids[i : i + WRITE_BATCH_SIZE]\n",
        "        vectorstore.add_documents(batch_docs, ids=batch_ids)\n",
        "\n",
        "    print(f\"\\nFinished. Collection: {COLLECTION_NAME}, new vectors in collection: {len(chunks)}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Populate vector storage**"
      ],
      "metadata": {
        "id": "z67kEAkykOER"
      },
      "id": "z67kEAkykOER"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f4298c66",
      "metadata": {
        "id": "f4298c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde26007-808a-4993-c43c-921ff0a18c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished. Collection: my_rag_collection, new vectors in collection: 13097\n"
          ]
        }
      ],
      "source": [
        "populate_vector_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b59b4fc",
      "metadata": {
        "id": "6b59b4fc"
      },
      "source": [
        "# Ask database (RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "207efc14",
      "metadata": {
        "id": "207efc14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple, Optional, Dict, Any, Iterable\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as qmodels\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "VpKHlszNlVjU"
      },
      "id": "VpKHlszNlVjU"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "25aea144",
      "metadata": {
        "id": "25aea144"
      },
      "outputs": [],
      "source": [
        "## Utils\n",
        "\n",
        "def _format_docs(docs: List[Document]) -> str:\n",
        "    \"\"\"Combines chunks into a single context with short [i] tags corresponding to sources.\"\"\"\n",
        "    parts: List[str] = []\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        src = d.metadata.get(\"source\", \"unknown\")\n",
        "        page = d.metadata.get(\"page\")\n",
        "        tag = f\"{src}\" + (f\":{page}\" if page is not None else \"\")\n",
        "        parts.append(f\"{d.page_content}\\n[[{i}] {tag}]\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts)\n",
        "\n",
        "\n",
        "def _pretty_sources(docs_with_scores: List[Tuple[Document, float]], topk: int = 4) -> List[str]:\n",
        "    \"\"\"Returns a unique list of sources (file[:page]) with score.\"\"\"\n",
        "    out: List[str] = []\n",
        "    seen = set()\n",
        "    for doc, score in docs_with_scores:\n",
        "        src = doc.metadata.get(\"source\", \"unknown\")\n",
        "        page = doc.metadata.get(\"page\")\n",
        "        label = f\"{src}\" + (f\":{page}\" if page is not None else \"\")\n",
        "        if label in seen:\n",
        "            continue\n",
        "        seen.add(label)\n",
        "        out.append(f\"{label} (score={score:.4f})\")\n",
        "        if len(out) >= topk:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "\n",
        "def _build_vectorstore(client: Optional[QdrantClient] = None) -> QdrantVectorStore:\n",
        "    \"\"\"Creates a VectorStore on an existing Qdrant collection.\"\"\"\n",
        "    client = QdrantClient(url=os.environ.get(\"QDRANT_URL\"), api_key= os.environ.get('QDRANT_API_KEY'), timeout=60)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "    vs = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME, embedding=embeddings)\n",
        "    return vs\n",
        "\n",
        "\n",
        "def build_retriever(\n",
        "    client: Optional[QdrantClient] = None,\n",
        "    mode: str = \"mmr\",\n",
        "    k: int = 6,\n",
        "    **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a retriever from QdrantVectorStore.\n",
        "    mode: \"mmr\" or \"similarity\"\n",
        "    k: number of chunks for the context\n",
        "    **kwargs: e.g. lambda_mult, fetch_k, filter (Qdrant Filter)\n",
        "    \"\"\"\n",
        "    vs = _build_vectorstore(client)\n",
        "    if mode == \"mmr\":\n",
        "        search_kwargs = {\"k\": k, \"fetch_k\": max(10, 3 * k), \"lambda_mult\": kwargs.pop(\"lambda_mult\", 0.5)}\n",
        "    else:\n",
        "        search_kwargs = {\"k\": k}\n",
        "    search_kwargs.update(kwargs)\n",
        "    return vs.as_retriever(search_type=mode, search_kwargs=search_kwargs)\n",
        "\n",
        "\n",
        "def _build_prompt_and_chain(retriever) -> Any:\n",
        "    \"\"\"Builds a simple RAG chain: retriever → prompt → ChatOpenAI → text.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\",\n",
        "             \"You are a helpful assistant. Answer concisely using ONLY the provided context. \"\n",
        "             \"If the answer is not in the context, say you don't know. \"\n",
        "             \"Use short citations like [1], [2] that correspond to the provided chunks.\"),\n",
        "            (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
        "        ]\n",
        "    )\n",
        "    llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": retriever | _format_docs,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function**"
      ],
      "metadata": {
        "id": "0h5eupj9lbBg"
      },
      "id": "0h5eupj9lbBg"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0d133707",
      "metadata": {
        "id": "0d133707"
      },
      "outputs": [],
      "source": [
        "def rag_answer(\n",
        "    question: str,\n",
        "    client: Optional[QdrantClient] = None,\n",
        "    mode: str = \"mmr\",\n",
        "    k: int = 6,\n",
        "    topk_sources: int = 4\n",
        ") -> Tuple[str, List[str]]:\n",
        "    \"\"\"Returns (answer, sources_list).\"\"\"\n",
        "    vs = _build_vectorstore(client)\n",
        "    retriever = build_retriever(client=client, mode=mode, k=k)\n",
        "    chain = _build_prompt_and_chain(retriever)\n",
        "\n",
        "    answer = chain.invoke(question)\n",
        "    topk_with_scores = vs.similarity_search_with_score(question, k=max(topk_sources, k))\n",
        "    sources = _pretty_sources(topk_with_scores, topk=topk_sources)\n",
        "    return answer, sources\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ask RAG**"
      ],
      "metadata": {
        "id": "cAoEYDV8lfTX"
      },
      "id": "cAoEYDV8lfTX"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0e92240d",
      "metadata": {
        "id": "0e92240d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7527e8d7-5845-4f67-dafc-f46dd30c9355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ANSWER ===\n",
            " **Capital Adequacy & Buffers Analysis through CAMELS Framework:**\n",
            "\n",
            "1. **CET1 Ratio**: The Common Equity Tier 1 (CET1) ratio has shown a declining trend from 6.175% in 2016 to 4.5% in 2020, indicating a potential weakening in capital adequacy over the years [1].\n",
            "\n",
            "2. **Tier 1 and Total Capital Ratios**: The Tier 1 capital ratio has increased from 1.825% in 2016 to 3.5% in 2020, while the total capital ratio has improved from 10.75% in 2016 to 14.3% in 2020, suggesting a strengthening of overall capital position despite the CET1 decline [1].\n",
            "\n",
            "3. **Leverage Ratio**: The BIS Tier 1 leverage ratio was reported at 5.5% as of the end of 2019, which is a critical measure of capital adequacy relative to total exposure [6].\n",
            "\n",
            "4. **Pillar 2 Requirements**: The capital management framework includes internal capital targets consistent with the risk profile, indicating adherence to Pillar 2 requirements [4].\n",
            "\n",
            "5. **MDA Headroom**: The analysis does not provide specific figures for MDA (Minimum Distributable Amount) headroom, so it is unclear how much buffer exists above the minimum requirements.\n",
            "\n",
            "6. **Buffer Adequacy vs. Requirements**: The capital conservation buffer has been maintained, with a requirement of 2.5% CET1 buffer under Basel III, which is crucial for absorbing losses during financial stress [5]. The overall capital strategy aims to optimize RWA usage while ensuring adequate buffers are in place [6].\n",
            "\n",
            "In summary, while there are signs of declining CET1 ratios, the overall capital strategy appears to be focused on maintaining a strong capital position through improved total capital ratios and adherence to regulatory requirements.\n",
            "\n",
            "=== SOURCES ===\n",
            "- data/NYSE_CS_2016.pdf:122 (score=0.5809)\n",
            "- data/NYSE_CS_2019.pdf:144 (score=0.5640)\n",
            "- data/NYSE_CS_2017.pdf:124 (score=0.5600)\n",
            "- data/NYSE_CS_2017.pdf:121 (score=0.5542)\n"
          ]
        }
      ],
      "source": [
        "ans, srcs = rag_answer(\"Focus your analysis through the CAMELS framework on these leading indicators - Capital adequacy & buffers: CET1, Tier 1, Total capital ratios, Leverage ratio, Pillar 2 requirements, MDA headroom, buffer adequacy vs. requirements\")\n",
        "print(\"\\n=== ANSWER ===\\n\", ans)\n",
        "print(\"\\n=== SOURCES ===\")\n",
        "for s in srcs:\n",
        "    print(\"-\", s)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}